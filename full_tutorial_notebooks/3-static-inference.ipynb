{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context specific (static) gene regulatory network (GRN) inference\n",
    "This notebooks runs the context specific GRN inference pipeline. It uses the input files prepared in `data` folder and saves context specific GRNs into a single file `output/static.h5`.\n",
    "\n",
    "The pipeline consists of several parts for platform (CPU of GPU) dependent optimization of computation speed. Each part is organized into two sections in this notebook:\n",
    "1. The execution section runs this part for network inference. You can see the commands and the output of each command.\n",
    "2. The command description section explains each command involved in this part.\n",
    "\n",
    "Depending on your mode of computation for pytorch (defined by `DEVICE` in `makefiles/config.mk`), each part may include more steps or should be totally skipped. **Read the instructions of each part carefully.** If you cannot find the description of a command in one part, search other parts.\n",
    "\n",
    "**You should change** `-j 32` option for `make` in every part to the number of parallel processes suitable for your machine. **It should be different** for CPU and GPU parts. Here the maximum number of cores used is `32*NTH=128`. `NTH` is set in `makefiles/config.mk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 CPU part 1, execution\n",
    "Here GPU is used to speed up computation. Therefore this part only infers the TF binding network that serves as a constraint for GRN inference.\n",
    "\n",
    "If you encounter errors from `make` like `No rule to make target...` or `Target 'cpu' not remade because of errors`, they can be safely ignored because these targets will be produced in CPU part 2. If you see other errors especially in peak or footprint detection steps due to low cell count for select cell subsets (typically several hundreds or lower), these errors can also be ignored and these cell subsets will be removed in the reconstructed networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes CPU usage limit by some jupyter versions\n",
    "import os\n",
    "os.environ['KMP_AFFINITY'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -eo pipefail\n",
    "cd ..\n",
    "#Run CPU part of inference\n",
    "#make -f makefiles/static.mk -j 32 -k cpu || true\n",
    "nohup make -f makefiles/static.mk -j 32 -k cpu || true > output.txt 2>&1 &\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 CPU part 1, command description\n",
    "### preproc selects_rna\n",
    "This command subsets the RNA data separately for cells of each context/cell type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys preproc selects_rna [-h] fi_reads fi_names fo_reads\n",
      "\n",
      "Select samples/cells based on external table for RNA data.\n",
      "\n",
      "positional arguments:\n",
      "  fi_reads    Path of input tsv file of full expression matrix\n",
      "  fi_names    Path of input text file of sample/cell names to select\n",
      "  fo_reads    Path of output tsv file of expression matrix of selected\n",
      "              samples/cells\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys preproc selects_rna -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preproc qc_reads\n",
    "This command removes low-read genes and cells for quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys preproc qc_reads [-h]\n",
      "                               fi_reads fo_reads n_gene nc_gene ncp_gene\n",
      "                               n_cell nt_cell ntp_cell\n",
      "\n",
      "Quality control by bounding read counts. Quality control is perform separately\n",
      "on genes based on their cell statisics and on cells based on their gene\n",
      "statistics, iteratively until dataset remains unchanged. A gene or cell is\n",
      "removed if any of the QC criteria is violated at any time in the iteration.\n",
      "All QC parameters can be set to 0 to disable QC filtering for that criterion.\n",
      "\n",
      "positional arguments:\n",
      "  fi_reads    Path of input tsv file of read count matrix. Rows are genes and\n",
      "              columns are cells.\n",
      "  fo_reads    Path of output tsv file of read count matrix after QC\n",
      "  n_gene      Lower bound on total read counts for gene QC\n",
      "  nc_gene     Lower bound on number of expressed cells for gene QC\n",
      "  ncp_gene    Lower bound on proportion of expressed cells for gene QC\n",
      "  n_cell      Lower bound on total read counts for cell QC\n",
      "  nt_cell     Lower bound on number of expressed genes for cell QC\n",
      "  ntp_cell    Lower bound on proportion of expressed genes for cell QC\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys preproc qc_reads -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preproc selects_atac\n",
    "This command combines per-cell bam files to a single bam file for each cell subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys preproc selects_atac [-h] fi_exp fi_list fo_list\n",
      "\n",
      "Select chromatin accessibility samples/cells based on presence in expression\n",
      "matrix.\n",
      "\n",
      "positional arguments:\n",
      "  fi_exp      Path of input tsv file of expression matrix. Column must be\n",
      "              sample/cell name.\n",
      "  fi_list     Path of input text file of selected cell names, one per line\n",
      "  fo_list     Path of output text file of selected cell names, one per line\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys preproc selects_atac -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chromatin macs2\n",
    "This command finds chromatin accessibility peaks with macs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys chromatin macs2 [-h] [--qcut QCUT] [--nth NTH] [--nmax NMAX]\n",
      "                              fi_names fi_bam fo_bam fo_bai fo_bed genome_size\n",
      "\n",
      "Peak calling using macs2. Needs bam files for each cell in a given folder.\n",
      "\n",
      "positional arguments:\n",
      "  fi_names     Path of input text file containing one sample/cell name per\n",
      "               line for macs2 peak calling\n",
      "  fi_bam       Path of input folder that contains each cell's bam file by name\n",
      "               in fi_names\n",
      "  fo_bam       Path of output bam file for select samples/cells\n",
      "  fo_bai       Path of output bai file for select samples/cells\n",
      "  fo_bed       Path of output bed file of peaks\n",
      "  genome_size  Genome size input of macs2. Use shortcuts hs or mm for human or\n",
      "               mouse.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help   show this help message and exit\n",
      "  --qcut QCUT  Qvalue cutoff for macs2 (default: 0.05)\n",
      "  --nth NTH    Number of threads (default: 1)\n",
      "  --nmax NMAX  Maximum number of peaks to retain, ordered by macs2 score. Use\n",
      "               0 for no limit. (default: 500000)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys chromatin macs2 -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chromatin wellington\n",
    "This command finds transcription factor footprints with wellington/pyDNase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys chromatin wellington [-h] [--fi_blacklist FI_BLACKLIST]\n",
      "                                   [--cut CUT] [--nth NTH] [--nmax NMAX]\n",
      "                                   fi_bam fi_bai fi_bed fo_bed\n",
      "\n",
      "TF footprinting with wellington.\n",
      "\n",
      "positional arguments:\n",
      "  fi_bam                Path of input bam file of all reads\n",
      "  fi_bai                Path of input bai file of all reads\n",
      "  fi_bed                Path of input bed file of peaks\n",
      "  fo_bed                Path of output bed file of footprints\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --fi_blacklist FI_BLACKLIST\n",
      "                        Path of input bed file of blacklisted genome regions\n",
      "                        to be removed (default: None)\n",
      "  --cut CUT             Cutoff for wellington score (default: 10)\n",
      "  --nth NTH             Number of threads (default: 1)\n",
      "  --nmax NMAX           Maximum number of footprints to retain, ordered by\n",
      "                        wellington score. Use 0 for no limit. (default:\n",
      "                        100000)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys chromatin wellington -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chromatin homer\n",
    "This command scans for motif occurrences within provided regions (here open chromatin peaks or footprints) with homer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys chromatin homer [-h] [--nth NTH]\n",
      "                              fi_bed fi_motif dirio_genome fi_exp fo_bed\n",
      "                              fo_wellington fo_homer\n",
      "\n",
      "Motif scan with homer.\n",
      "\n",
      "positional arguments:\n",
      "  fi_bed         Path of input bed file of regions\n",
      "  fi_motif       Path of input motif PWM file in homer format. Motifs must be\n",
      "                 named in format 'gene...' where gene matches gene names in\n",
      "                 fi_exp. Should not contain duplicates.\n",
      "  dirio_genome   Path of input & output folder or file for reference genome\n",
      "                 for homer. A separate hard copy is recommended because homer\n",
      "                 may write into the folder to preparse genome.\n",
      "  fi_exp         Path of input expression matrix file in tsv format. Used for\n",
      "                 mapping motifs to genes.\n",
      "  fo_bed         Path of output bed file of detected motifs\n",
      "  fo_wellington  Path of output tsv file of wellington scores in shape\n",
      "                 (region,motif)\n",
      "  fo_homer       Path of output tsv file of homer scores in shape\n",
      "                 (region,motif)\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help     show this help message and exit\n",
      "  --nth NTH      Number of threads (default: 1)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys chromatin homer -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chromatin binding\n",
    "This command computes an integrative score for TF binding based on scores from footprint/peak discovery and from homer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys chromatin binding [-h] [--cuth CUTH] [--cutw CUTW] [--cut CUT]\n",
      "                                [--combine COMBINE] [--mode MODE]\n",
      "                                fi_wellington fi_homer fo_bind\n",
      "\n",
      "Finding TF binding events. Combines wellington and homer outputs to infer TF\n",
      "binding events by merging motifs to TFs.\n",
      "\n",
      "positional arguments:\n",
      "  fi_wellington      Path of input tsv file of wellington output\n",
      "  fi_homer           Path of input tsv file of homer output\n",
      "  fo_bind            Path of output tsv file of binding events\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help         show this help message and exit\n",
      "  --cuth CUTH        Homer score cutoff (default: 0)\n",
      "  --cutw CUTW        Wellington score cutoff (default: 0)\n",
      "  --cut CUT          Final score (integrating homer & wellington) cutoff\n",
      "                     (default: None)\n",
      "  --combine COMBINE  Method to combine scores of motifs of the same TF.\n",
      "                     Accepts: max, mean, sum. (default: max)\n",
      "  --mode MODE        Mode to compute final score. Accepts binary flags: 1: Add\n",
      "                     log(wellington score) 2: Add log(homer score) 4: Subtract\n",
      "                     log(10)*(distance_to_tss)/1E6 (default: 3)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys chromatin binding -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chromatin tssdist\n",
    "This command computes the distance between each (open chromatin) region and each gene's transcription start site to prioritize nearby pairs that are more likely to have regulatory effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys chromatin tssdist [-h] [--cut CUT] [--nmin NMIN] [--nmax NMAX]\n",
      "                                fi_exp fi_wellington fi_tss fo_dist\n",
      "\n",
      "Annotating TF bond regions to target genes based on distance to TSS.\n",
      "\n",
      "positional arguments:\n",
      "  fi_exp         Path of input expression matrix file in tsv format to obtain\n",
      "                 gene names\n",
      "  fi_wellington  Path of input tsv file of wellington scores to obtain DNA\n",
      "                 regions\n",
      "  fi_tss         Path of input bed file for gene region and strand\n",
      "  fo_dist        Path of output tsv file of distance from TF-bond regions to\n",
      "                 TSS\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help     show this help message and exit\n",
      "  --cut CUT      Distance cutoff between DNA region and target gene TSS\n",
      "                 (default: 500000)\n",
      "  --nmin NMIN    Minimal total number of links to recover (default: 1)\n",
      "  --nmax NMAX    Maximal total number of links to recover (default: 10000000)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys chromatin tssdist -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chromatin linking\n",
    "This command links TFs to its potential target genes via the relation: TF --- motif --- region --- gene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys chromatin linking [-h] [--fi_whitelist FI_WHITELIST]\n",
      "                                [--whitelist_mode WHITELIST_MODE]\n",
      "                                [--combine COMBINE] [--mode MODE]\n",
      "                                fi_binding fi_dist fo_linking\n",
      "\n",
      "Linking regulators and targets with scores.\n",
      "\n",
      "positional arguments:\n",
      "  fi_binding            Path of input tsv file of binding events\n",
      "  fi_dist               Path of input tsv file of distance from TF-bond\n",
      "                        regions to TSS\n",
      "  fo_linking            Path of output matrix file of TF to potential target\n",
      "                        gene link scores\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --fi_whitelist FI_WHITELIST\n",
      "                        Path of input bed file of potential regulatory target\n",
      "                        genes of each region. The fourth column (or its first\n",
      "                        item after split by _) should be taget gene name. Can\n",
      "                        be used to filter regulatory regions based on co-\n",
      "                        accessibility or association with target gene\n",
      "                        expression. (default: None)\n",
      "  --whitelist_mode WHITELIST_MODE\n",
      "                        Criterion of whitelist. Accepts: intersect: the region\n",
      "                        must intersect with a whitelisted region (default)\n",
      "                        within: the region must be inside a whitelisted region\n",
      "                        (default: intersect)\n",
      "  --combine COMBINE     Method to combine scores of motifs of the same TF.\n",
      "                        Accepts: max, mean, sum. (default: max)\n",
      "  --mode MODE           Mode to compute final score. Accepts binary flags: 4:\n",
      "                        Subtract log(10)*(distance_to_tss)/1E6 (default: 4)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys chromatin linking -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chromatin binlinking\n",
    "This command selects the strongest TF-target gene pairs as a TF binding network that constrains the GRN to be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys chromatin binlinking [-h] [--axis AXIS] [--selfreg SELFREG]\n",
      "                                   [--inf INF]\n",
      "                                   fi_linking fo_binlinking n\n",
      "\n",
      "Converting regulator-target link score matrix to binary. Chooses the top\n",
      "regulator-target links, separately for each target gene by default.\n",
      "\n",
      "positional arguments:\n",
      "  fi_linking         Path of input matrix file of TF to potential target gene\n",
      "                     link scores\n",
      "  fo_binlinking      Path of output matrix file of TF to potential target gene\n",
      "                     links\n",
      "  n                  Number of regulator-target links. n strongest links (with\n",
      "                     highest scores) are selected along axis axis. If greater\n",
      "                     than the maximum links available, all links will be\n",
      "                     selected subject to inf parameter constraint. Value -1\n",
      "                     selects all non-inf links.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help         show this help message and exit\n",
      "  --axis AXIS        Axis to choose the top links. If None, n links in total\n",
      "                     are selected among all regulator-target links. Defaults\n",
      "                     to 1, indicating n strongest links/regulators are\n",
      "                     selected for each target. (default: 1)\n",
      "  --selfreg SELFREG  How to handle self regulation. Accepts: error: Raise\n",
      "                     ValueError if seen in fi_linking (default: error)\n",
      "  --inf INF          Whether to select links with -inf score. Accepts: never.\n",
      "                     (default: never)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys chromatin binlinking -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 GPU part, execution\n",
    "**This part should be skipped if you use CPU for pytorch.**\n",
    "\n",
    "This part performs GRN inference with scRNA-seq read counts on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -eo pipefail\n",
    "cd ..\n",
    "#make -f makefiles/static.mk -j 2 -k gpu || true\n",
    "nohup make -f makefiles/static.mk -j 2 -k gpu || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GPU part, command description\n",
    "### network reconstruct\n",
    "This command uses pyro and pytorch to infer the GRN with stochastic process model under the TF binding network constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys network reconstruct [-h] [--lr LR] [--lrd LRD] [--nstep NSTEP]\n",
      "                                  [--npc NPC] [--fi_cov FI_COV]\n",
      "                                  [--model MODEL]\n",
      "                                  [--nstep_report NSTEP_REPORT]\n",
      "                                  [--rseed RSEED] [--device DEVICE]\n",
      "                                  [--dtype DTYPE] [--loss LOSS] [--nth NTH]\n",
      "                                  [--varmean VARMEAN] [--varstd VARSTD]\n",
      "                                  [--fo_weightz FO_WEIGHTZ]\n",
      "                                  [--scale_lyapunov SCALE_LYAPUNOV]\n",
      "                                  fi_exp fi_mask fo_weight fo_meanvar\n",
      "                                  fo_covfactor fo_loss fo_stats\n",
      "\n",
      "Reconstruct network with any pyro model in net_pyro_models that is based on\n",
      "covariance_model and has binary masks.\n",
      "\n",
      "positional arguments:\n",
      "  fi_exp                Path of input tsv file of expression matrix.\n",
      "  fi_mask               Path of input tsv file of mask matrix indicating which\n",
      "                        edges are allowed. Can be output of dictys chromatin\n",
      "                        binlinking.\n",
      "  fo_weight             Path of output tsv file of edge weight matrix\n",
      "  fo_meanvar            Path of output tsv file of mean and variance of each\n",
      "                        gene's relative log expression\n",
      "  fo_covfactor          Path of output tsv file of factors for the off-\n",
      "                        diagonal component of gene covariance matrix\n",
      "  fo_loss               Path of output tsv file of sublosses in each training\n",
      "                        step\n",
      "  fo_stats              Path of output tsv file of basic stats of each\n",
      "                        variable during training\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --lr LR               Initial iearning rate (default: 0.01)\n",
      "  --lrd LRD             Learning rate decay (default: 0.999)\n",
      "  --nstep NSTEP         Number of training steps (default: 4000)\n",
      "  --npc NPC             Number of unknown factors for covariance in\n",
      "                        multivariate distribution (default: 0)\n",
      "  --fi_cov FI_COV       Path of input tsv file of covariate matrix for each\n",
      "                        cell to be included. Should have cell x covariate\n",
      "                        shape. (default: None)\n",
      "  --model MODEL         Name of model to train (default: ou)\n",
      "  --nstep_report NSTEP_REPORT\n",
      "                        Number of steps to save each parameter distribution\n",
      "                        (default: 100)\n",
      "  --rseed RSEED         Initial random seed (default: 12345)\n",
      "  --device DEVICE       Device for pytorch and pyro. See TBA.... (default:\n",
      "                        cpu)\n",
      "  --dtype DTYPE         Data type for pytorch. See TBA... (default: float)\n",
      "  --loss LOSS           Loss function name (default: Trace_ELBO_site)\n",
      "  --nth NTH             Number of threads for CPU usage. When <1, use\n",
      "                        nth*(detected core count). (default: 1)\n",
      "  --varmean VARMEAN     Pyro parameter name for mean of effect size (default:\n",
      "                        N_0val)\n",
      "  --varstd VARSTD       Pyro parameter name for std of effect size to compute\n",
      "                        z scores. Use None if unavailable. (default: None)\n",
      "  --fo_weightz FO_WEIGHTZ\n",
      "                        Path of output tsv file of z score matrix of edge\n",
      "                        weights (default: None)\n",
      "  --scale_lyapunov SCALE_LYAPUNOV\n",
      "                        Scale of Lyapunov equation inequality loss (default:\n",
      "                        100000.0)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys network reconstruct -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 CPU part 2, execution\n",
    "**This part should be skipped if you use CPU for pytorch.**\n",
    "\n",
    "This part performs network postprocessing to address variance estimation bias and indirect effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -eo pipefail\n",
    "cd ..\n",
    "#make -f makefiles/static.mk -j 32 -k cpu || true\n",
    "nohup make -f makefiles/static.mk -j 32 -k cpu || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 CPU part 2, command description\n",
    "### network normalize\n",
    "This command normalizes network edges based on the standard deviation of regulator and target genes. This can overcome biases in the estimation of variance of gene expression due to single-cell sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys network normalize [-h] [--norm NORM] [--nth NTH]\n",
      "                                fi_weight fi_meanvar fi_covfactor fo_nweight\n",
      "\n",
      "Normalize edge strength. So they are more resistant to estimation bias of true\n",
      "expression level variance.\n",
      "\n",
      "positional arguments:\n",
      "  fi_weight     Path of input tsv file of edge weight matrix\n",
      "  fi_meanvar    Path of iput tsv file of mean and variance of each gene's\n",
      "                relative log expression\n",
      "  fi_covfactor  Path of iput tsv file of factors for the off-diagonal\n",
      "                component of gene covariance matrix\n",
      "  fo_nweight    Path of output tsv file of normalized edge weight matrix\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help    show this help message and exit\n",
      "  --norm NORM   Type of normalization as binary flag values. Accepts: 1:\n",
      "                Multiplying edge weight with stochastic noise std of TF 2:\n",
      "                Dividing edge weight with stochastic noise std of target\n",
      "                (default: 3)\n",
      "  --nth NTH     Number of threads for CPU usage. When <1, use nth*(detected\n",
      "                core count). (default: 1)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys network normalize -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network indirect\n",
    "This command computes the steady-state total effect (direct + indirect effects) networks from the kinetic direct effect network. The total effect network is **not yet integrated into network analysis functions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys network indirect [-h] [--norm NORM] [--fi_meanvar FI_MEANVAR]\n",
      "                               [--eigmin EIGMIN] [--eigmax EIGMAX]\n",
      "                               [--multiplier MULTIPLIER] [--nth NTH]\n",
      "                               fi_weight fi_covfactor fo_iweight\n",
      "\n",
      "Computes steady-state indirect effect of gene perturbation from OU process.\n",
      "Performs extra regularization on network by bounding the eigenvalues of\n",
      "feedback loops with parameters eigmin and eigmax. Values away from 1 indicates\n",
      "stronger feedback loop effects. Set values closer to 1 to apply stronger\n",
      "regularization.\n",
      "\n",
      "positional arguments:\n",
      "  fi_weight             Path of input tsv file of edge weight matrix\n",
      "  fi_covfactor          Path of iput tsv file of factors for the off-diagonal\n",
      "                        component of gene covariance matrix\n",
      "  fo_iweight            Path of output tsv file of steady-state indirect\n",
      "                        effect edge weight matrix\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --norm NORM           Normalizations of indirect effect matrix. Binary flag\n",
      "                        variable that accepts: 1: Divides the indirect effect\n",
      "                        size of each TF on other genes with that on itself to\n",
      "                        obtain logFC per unit logFC of TF. 2: Accounts for\n",
      "                        mRNA saturation/sampling effects. (default: 3)\n",
      "  --fi_meanvar FI_MEANVAR\n",
      "                        Path of iput tsv file of mean and variance of each\n",
      "                        gene's relative log expression. Needed if norm & 2.\n",
      "                        (default: None)\n",
      "  --eigmin EIGMIN       Lower bound of eigenvalue for finding the minimal\n",
      "                        required extra regularization. (default: 0.2)\n",
      "  --eigmax EIGMAX       Upper bound of eigenvalue for finding the minimal\n",
      "                        required extra regularization. (default: 1.8)\n",
      "  --multiplier MULTIPLIER\n",
      "                        Step size of strengthening regularization. Larger\n",
      "                        values provide faster but over-regularization. Must be\n",
      "                        greater than 1. (default: 1.1)\n",
      "  --nth NTH             Number of threads for CPU usage. When <1, use\n",
      "                        nth*(detected core count). (default: 1)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys network indirect -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Aggregating network part, execution\n",
    "This part aggregates all the inferred networks into a single h5 file as output. This single file can be shared and processed by network analysis/visualization functions in Dictys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'combine'.\n",
      "rm -f tmp_static/Subset1/names_rna.txt tmp_static/Subset1/names_atac0.txt tmp_static/Subset1/names_atac.txt tmp_static/Subset1/expression0.tsv.gz tmp_static/Subset1/expression.tsv.gz tmp_static/Subset1/names_atac.txt tmp_static/Subset1/reads.bam tmp_static/Subset1/reads.bai tmp_static/Subset1/peaks.bed tmp_static/Subset1/footprints.bed tmp_static/Subset1/motifs.bed tmp_static/Subset1/homer.tsv.gz tmp_static/Subset1/wellington.tsv.gz tmp_static/Subset1/binding.tsv.gz tmp_static/Subset1/tssdist.tsv.gz tmp_static/Subset1/linking.tsv.gz tmp_static/Subset1/binlinking.tsv.gz tmp_static/Subset1/net_nweight.tsv.gz tmp_static/Subset1/net_iweight.tsv.gz tmp_static/Subset1/net_inweight.tsv.gz tmp_static/Subset10/names_rna.txt tmp_static/Subset10/names_atac0.txt tmp_static/Subset10/names_atac.txt tmp_static/Subset10/expression0.tsv.gz tmp_static/Subset10/expression.tsv.gz tmp_static/Subset10/names_atac.txt tmp_static/Subset10/reads.bam tmp_static/Subset10/reads.bai tmp_static/Subset10/peaks.bed tmp_static/Subset10/footprints.bed tmp_static/Subset10/motifs.bed tmp_static/Subset10/homer.tsv.gz tmp_static/Subset10/wellington.tsv.gz tmp_static/Subset10/binding.tsv.gz tmp_static/Subset10/tssdist.tsv.gz tmp_static/Subset10/linking.tsv.gz tmp_static/Subset10/binlinking.tsv.gz tmp_static/Subset10/net_nweight.tsv.gz tmp_static/Subset10/net_iweight.tsv.gz tmp_static/Subset10/net_inweight.tsv.gz tmp_static/Subset11/names_rna.txt tmp_static/Subset11/names_atac0.txt tmp_static/Subset11/names_atac.txt tmp_static/Subset11/expression0.tsv.gz tmp_static/Subset11/expression.tsv.gz tmp_static/Subset11/names_atac.txt tmp_static/Subset11/reads.bam tmp_static/Subset11/reads.bai tmp_static/Subset11/peaks.bed tmp_static/Subset11/footprints.bed tmp_static/Subset11/motifs.bed tmp_static/Subset11/homer.tsv.gz tmp_static/Subset11/wellington.tsv.gz tmp_static/Subset11/binding.tsv.gz tmp_static/Subset11/tssdist.tsv.gz tmp_static/Subset11/linking.tsv.gz tmp_static/Subset11/binlinking.tsv.gz tmp_static/Subset11/net_nweight.tsv.gz tmp_static/Subset11/net_iweight.tsv.gz tmp_static/Subset11/net_inweight.tsv.gz tmp_static/Subset12/names_rna.txt tmp_static/Subset12/names_atac0.txt tmp_static/Subset12/names_atac.txt tmp_static/Subset12/expression0.tsv.gz tmp_static/Subset12/expression.tsv.gz tmp_static/Subset12/names_atac.txt tmp_static/Subset12/reads.bam tmp_static/Subset12/reads.bai tmp_static/Subset12/peaks.bed tmp_static/Subset12/footprints.bed tmp_static/Subset12/motifs.bed tmp_static/Subset12/homer.tsv.gz tmp_static/Subset12/wellington.tsv.gz tmp_static/Subset12/binding.tsv.gz tmp_static/Subset12/tssdist.tsv.gz tmp_static/Subset12/linking.tsv.gz tmp_static/Subset12/binlinking.tsv.gz tmp_static/Subset12/net_nweight.tsv.gz tmp_static/Subset12/net_iweight.tsv.gz tmp_static/Subset12/net_inweight.tsv.gz tmp_static/Subset13/names_rna.txt tmp_static/Subset13/names_atac0.txt tmp_static/Subset13/names_atac.txt tmp_static/Subset13/expression0.tsv.gz tmp_static/Subset13/expression.tsv.gz tmp_static/Subset13/names_atac.txt tmp_static/Subset13/reads.bam tmp_static/Subset13/reads.bai tmp_static/Subset13/peaks.bed tmp_static/Subset13/footprints.bed tmp_static/Subset13/motifs.bed tmp_static/Subset13/homer.tsv.gz tmp_static/Subset13/wellington.tsv.gz tmp_static/Subset13/binding.tsv.gz tmp_static/Subset13/tssdist.tsv.gz tmp_static/Subset13/linking.tsv.gz tmp_static/Subset13/binlinking.tsv.gz tmp_static/Subset13/net_nweight.tsv.gz tmp_static/Subset13/net_iweight.tsv.gz tmp_static/Subset13/net_inweight.tsv.gz tmp_static/Subset14/names_rna.txt tmp_static/Subset14/names_atac0.txt tmp_static/Subset14/names_atac.txt tmp_static/Subset14/expression0.tsv.gz tmp_static/Subset14/expression.tsv.gz tmp_static/Subset14/names_atac.txt tmp_static/Subset14/reads.bam tmp_static/Subset14/reads.bai tmp_static/Subset14/peaks.bed tmp_static/Subset14/footprints.bed tmp_static/Subset14/motifs.bed tmp_static/Subset14/homer.tsv.gz tmp_static/Subset14/wellington.tsv.gz tmp_static/Subset14/binding.tsv.gz tmp_static/Subset14/tssdist.tsv.gz tmp_static/Subset14/linking.tsv.gz tmp_static/Subset14/binlinking.tsv.gz tmp_static/Subset14/net_nweight.tsv.gz tmp_static/Subset14/net_iweight.tsv.gz tmp_static/Subset14/net_inweight.tsv.gz tmp_static/Subset2/names_rna.txt tmp_static/Subset2/names_atac0.txt tmp_static/Subset2/names_atac.txt tmp_static/Subset2/expression0.tsv.gz tmp_static/Subset2/expression.tsv.gz tmp_static/Subset2/names_atac.txt tmp_static/Subset2/reads.bam tmp_static/Subset2/reads.bai tmp_static/Subset2/peaks.bed tmp_static/Subset2/footprints.bed tmp_static/Subset2/motifs.bed tmp_static/Subset2/homer.tsv.gz tmp_static/Subset2/wellington.tsv.gz tmp_static/Subset2/binding.tsv.gz tmp_static/Subset2/tssdist.tsv.gz tmp_static/Subset2/linking.tsv.gz tmp_static/Subset2/binlinking.tsv.gz tmp_static/Subset2/net_nweight.tsv.gz tmp_static/Subset2/net_iweight.tsv.gz tmp_static/Subset2/net_inweight.tsv.gz tmp_static/Subset3/names_rna.txt tmp_static/Subset3/names_atac0.txt tmp_static/Subset3/names_atac.txt tmp_static/Subset3/expression0.tsv.gz tmp_static/Subset3/expression.tsv.gz tmp_static/Subset3/names_atac.txt tmp_static/Subset3/reads.bam tmp_static/Subset3/reads.bai tmp_static/Subset3/peaks.bed tmp_static/Subset3/footprints.bed tmp_static/Subset3/motifs.bed tmp_static/Subset3/homer.tsv.gz tmp_static/Subset3/wellington.tsv.gz tmp_static/Subset3/binding.tsv.gz tmp_static/Subset3/tssdist.tsv.gz tmp_static/Subset3/linking.tsv.gz tmp_static/Subset3/binlinking.tsv.gz tmp_static/Subset3/net_nweight.tsv.gz tmp_static/Subset3/net_iweight.tsv.gz tmp_static/Subset3/net_inweight.tsv.gz tmp_static/Subset4/names_rna.txt tmp_static/Subset4/names_atac0.txt tmp_static/Subset4/names_atac.txt tmp_static/Subset4/expression0.tsv.gz tmp_static/Subset4/expression.tsv.gz tmp_static/Subset4/names_atac.txt tmp_static/Subset4/reads.bam tmp_static/Subset4/reads.bai tmp_static/Subset4/peaks.bed tmp_static/Subset4/footprints.bed tmp_static/Subset4/motifs.bed tmp_static/Subset4/homer.tsv.gz tmp_static/Subset4/wellington.tsv.gz tmp_static/Subset4/binding.tsv.gz tmp_static/Subset4/tssdist.tsv.gz tmp_static/Subset4/linking.tsv.gz tmp_static/Subset4/binlinking.tsv.gz tmp_static/Subset4/net_nweight.tsv.gz tmp_static/Subset4/net_iweight.tsv.gz tmp_static/Subset4/net_inweight.tsv.gz tmp_static/Subset5/names_rna.txt tmp_static/Subset5/names_atac0.txt tmp_static/Subset5/names_atac.txt tmp_static/Subset5/expression0.tsv.gz tmp_static/Subset5/expression.tsv.gz tmp_static/Subset5/names_atac.txt tmp_static/Subset5/reads.bam tmp_static/Subset5/reads.bai tmp_static/Subset5/peaks.bed tmp_static/Subset5/footprints.bed tmp_static/Subset5/motifs.bed tmp_static/Subset5/homer.tsv.gz tmp_static/Subset5/wellington.tsv.gz tmp_static/Subset5/binding.tsv.gz tmp_static/Subset5/tssdist.tsv.gz tmp_static/Subset5/linking.tsv.gz tmp_static/Subset5/binlinking.tsv.gz tmp_static/Subset5/net_nweight.tsv.gz tmp_static/Subset5/net_iweight.tsv.gz tmp_static/Subset5/net_inweight.tsv.gz tmp_static/Subset6/names_rna.txt tmp_static/Subset6/names_atac0.txt tmp_static/Subset6/names_atac.txt tmp_static/Subset6/expression0.tsv.gz tmp_static/Subset6/expression.tsv.gz tmp_static/Subset6/names_atac.txt tmp_static/Subset6/reads.bam tmp_static/Subset6/reads.bai tmp_static/Subset6/peaks.bed tmp_static/Subset6/footprints.bed tmp_static/Subset6/motifs.bed tmp_static/Subset6/homer.tsv.gz tmp_static/Subset6/wellington.tsv.gz tmp_static/Subset6/binding.tsv.gz tmp_static/Subset6/tssdist.tsv.gz tmp_static/Subset6/linking.tsv.gz tmp_static/Subset6/binlinking.tsv.gz tmp_static/Subset6/net_nweight.tsv.gz tmp_static/Subset6/net_iweight.tsv.gz tmp_static/Subset6/net_inweight.tsv.gz tmp_static/Subset7/names_rna.txt tmp_static/Subset7/names_atac0.txt tmp_static/Subset7/names_atac.txt tmp_static/Subset7/expression0.tsv.gz tmp_static/Subset7/expression.tsv.gz tmp_static/Subset7/names_atac.txt tmp_static/Subset7/reads.bam tmp_static/Subset7/reads.bai tmp_static/Subset7/peaks.bed tmp_static/Subset7/footprints.bed tmp_static/Subset7/motifs.bed tmp_static/Subset7/homer.tsv.gz tmp_static/Subset7/wellington.tsv.gz tmp_static/Subset7/binding.tsv.gz tmp_static/Subset7/tssdist.tsv.gz tmp_static/Subset7/linking.tsv.gz tmp_static/Subset7/binlinking.tsv.gz tmp_static/Subset7/net_nweight.tsv.gz tmp_static/Subset7/net_iweight.tsv.gz tmp_static/Subset7/net_inweight.tsv.gz tmp_static/Subset8/names_rna.txt tmp_static/Subset8/names_atac0.txt tmp_static/Subset8/names_atac.txt tmp_static/Subset8/expression0.tsv.gz tmp_static/Subset8/expression.tsv.gz tmp_static/Subset8/names_atac.txt tmp_static/Subset8/reads.bam tmp_static/Subset8/reads.bai tmp_static/Subset8/peaks.bed tmp_static/Subset8/footprints.bed tmp_static/Subset8/motifs.bed tmp_static/Subset8/homer.tsv.gz tmp_static/Subset8/wellington.tsv.gz tmp_static/Subset8/binding.tsv.gz tmp_static/Subset8/tssdist.tsv.gz tmp_static/Subset8/linking.tsv.gz tmp_static/Subset8/binlinking.tsv.gz tmp_static/Subset8/net_nweight.tsv.gz tmp_static/Subset8/net_iweight.tsv.gz tmp_static/Subset8/net_inweight.tsv.gz tmp_static/Subset9/names_rna.txt tmp_static/Subset9/names_atac0.txt tmp_static/Subset9/names_atac.txt tmp_static/Subset9/expression0.tsv.gz tmp_static/Subset9/expression.tsv.gz tmp_static/Subset9/names_atac.txt tmp_static/Subset9/reads.bam tmp_static/Subset9/reads.bai tmp_static/Subset9/peaks.bed tmp_static/Subset9/footprints.bed tmp_static/Subset9/motifs.bed tmp_static/Subset9/homer.tsv.gz tmp_static/Subset9/wellington.tsv.gz tmp_static/Subset9/binding.tsv.gz tmp_static/Subset9/tssdist.tsv.gz tmp_static/Subset9/linking.tsv.gz tmp_static/Subset9/binlinking.tsv.gz tmp_static/Subset9/net_nweight.tsv.gz tmp_static/Subset9/net_iweight.tsv.gz tmp_static/Subset9/net_inweight.tsv.gz tmp_static/Subset1/net_weight.tsv.gz tmp_static/Subset1/net_meanvar.tsv.gz tmp_static/Subset1/net_covfactor.tsv.gz tmp_static/Subset1/net_loss.tsv.gz tmp_static/Subset1/net_stats.tsv.gz tmp_static/Subset10/net_weight.tsv.gz tmp_static/Subset10/net_meanvar.tsv.gz tmp_static/Subset10/net_covfactor.tsv.gz tmp_static/Subset10/net_loss.tsv.gz tmp_static/Subset10/net_stats.tsv.gz tmp_static/Subset11/net_weight.tsv.gz tmp_static/Subset11/net_meanvar.tsv.gz tmp_static/Subset11/net_covfactor.tsv.gz tmp_static/Subset11/net_loss.tsv.gz tmp_static/Subset11/net_stats.tsv.gz tmp_static/Subset12/net_weight.tsv.gz tmp_static/Subset12/net_meanvar.tsv.gz tmp_static/Subset12/net_covfactor.tsv.gz tmp_static/Subset12/net_loss.tsv.gz tmp_static/Subset12/net_stats.tsv.gz tmp_static/Subset13/net_weight.tsv.gz tmp_static/Subset13/net_meanvar.tsv.gz tmp_static/Subset13/net_covfactor.tsv.gz tmp_static/Subset13/net_loss.tsv.gz tmp_static/Subset13/net_stats.tsv.gz tmp_static/Subset14/net_weight.tsv.gz tmp_static/Subset14/net_meanvar.tsv.gz tmp_static/Subset14/net_covfactor.tsv.gz tmp_static/Subset14/net_loss.tsv.gz tmp_static/Subset14/net_stats.tsv.gz tmp_static/Subset2/net_weight.tsv.gz tmp_static/Subset2/net_meanvar.tsv.gz tmp_static/Subset2/net_covfactor.tsv.gz tmp_static/Subset2/net_loss.tsv.gz tmp_static/Subset2/net_stats.tsv.gz tmp_static/Subset3/net_weight.tsv.gz tmp_static/Subset3/net_meanvar.tsv.gz tmp_static/Subset3/net_covfactor.tsv.gz tmp_static/Subset3/net_loss.tsv.gz tmp_static/Subset3/net_stats.tsv.gz tmp_static/Subset4/net_weight.tsv.gz tmp_static/Subset4/net_meanvar.tsv.gz tmp_static/Subset4/net_covfactor.tsv.gz tmp_static/Subset4/net_loss.tsv.gz tmp_static/Subset4/net_stats.tsv.gz tmp_static/Subset5/net_weight.tsv.gz tmp_static/Subset5/net_meanvar.tsv.gz tmp_static/Subset5/net_covfactor.tsv.gz tmp_static/Subset5/net_loss.tsv.gz tmp_static/Subset5/net_stats.tsv.gz tmp_static/Subset6/net_weight.tsv.gz tmp_static/Subset6/net_meanvar.tsv.gz tmp_static/Subset6/net_covfactor.tsv.gz tmp_static/Subset6/net_loss.tsv.gz tmp_static/Subset6/net_stats.tsv.gz tmp_static/Subset7/net_weight.tsv.gz tmp_static/Subset7/net_meanvar.tsv.gz tmp_static/Subset7/net_covfactor.tsv.gz tmp_static/Subset7/net_loss.tsv.gz tmp_static/Subset7/net_stats.tsv.gz tmp_static/Subset8/net_weight.tsv.gz tmp_static/Subset8/net_meanvar.tsv.gz tmp_static/Subset8/net_covfactor.tsv.gz tmp_static/Subset8/net_loss.tsv.gz tmp_static/Subset8/net_stats.tsv.gz tmp_static/Subset9/net_weight.tsv.gz tmp_static/Subset9/net_meanvar.tsv.gz tmp_static/Subset9/net_covfactor.tsv.gz tmp_static/Subset9/net_loss.tsv.gz tmp_static/Subset9/net_stats.tsv.gz               \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -eo pipefail\n",
    "cd ..\n",
    "#Combine inferred networks to single h5 file\n",
    "make -f makefiles/static.mk combine\n",
    "#Optional step: Cleanup intermediate files\n",
    "make -f makefiles/static.mk clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Aggregating network part, command description\n",
    "### network tofile\n",
    "This command aggregates all inferred networks to a single output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dictys network tofile [-h] [--dynamic] [--nettype NETTYPE]\n",
      "                             [--optional OPTIONAL] [--fi_c FI_C]\n",
      "                             diri_data diri_work fi_subsets fo_networks\n",
      "\n",
      "Saving networks to a single file.\n",
      "\n",
      "positional arguments:\n",
      "  diri_data            Path of input data folder to load from\n",
      "  diri_work            Path of input working folder to load from\n",
      "  fi_subsets           Path of input txt file for cell subset names\n",
      "  fo_networks          Path of output h5 file for all networks\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help           show this help message and exit\n",
      "  --dynamic            Whether to load a dynamic network instead of a set of\n",
      "                       static networks (default: False)\n",
      "  --nettype NETTYPE    Type of network. Accepts: '': Unnormalized direct\n",
      "                       network 'n': Normalized direct network 'i':\n",
      "                       Unnormalized steady-state network 'in': Normalized\n",
      "                       steady-state network (default: n)\n",
      "  --optional OPTIONAL  Optional data to include. Accepts: readcount: RNA read\n",
      "                       count for each cell (default: readcount)\n",
      "  --fi_c FI_C          Path of input tsv file for extra property columns for\n",
      "                       each cell (default: None)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m dictys network tofile -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
